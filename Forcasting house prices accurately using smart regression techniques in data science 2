import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Install required libraries
!pip install pandas numpy scikit-learn matplotlib seaborn plotly scipy

# Set random seed for reproducibility
np.random.seed(42)

# 1. Load the dataset
url = "https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv"
df = pd.read_csv(url)

# 2. Data Preprocessing
df['total_bedrooms'] = df['total_bedrooms'].fillna(df['total_bedrooms'].median())
df.drop_duplicates(inplace=True)

def cap_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return series.clip(lower_bound, upper_bound)

df['median_house_value'] = cap_outliers(df['median_house_value'])
df['total_rooms'] = cap_outliers(df['total_rooms'])
df['median_house_value'] = np.log1p(df['median_house_value'])
df = df.drop('ocean_proximity', axis=1)

# 3. Feature Engineering
df['rooms_per_household'] = df['total_rooms'] / df['households']
df['distance_to_coast'] = np.abs(df['longitude'] + 122)
poly = PolynomialFeatures(degree=2, include_bias=False)
median_income_poly = poly.fit_transform(df[['median_income']])
df['median_income_poly1'] = median_income_poly[:, 1]

# 4. Statistical Analysis
def statistical_analysis(df):
    stat, p_value = stats.shapiro(df['median_house_value'])
    normality_result = f"Shapiro-Wilk Test for median_house_value: p-value = {p_value:.4f}"
    desc_stats = df.describe().T
    desc_stats['skewness'] = df.skew()
    desc_stats['kurtosis'] = df.kurtosis()
    return normality_result, desc_stats

normality_result, desc_stats = statistical_analysis(df)
print("\nStatistical Analysis:")
print(normality_result)
print("\nDescriptive Statistics with Skewness and Kurtosis:")
print(desc_stats)

# 5. Exploratory Data Analysis (EDA)
corr_matrix = df.corr()
fig_corr = go.Figure(data=go.Heatmap(
    z=corr_matrix.values,
    x=corr_matrix.columns,
    y=corr_matrix.columns,
    colorscale='RdBu',
    zmin=-1, zmax=1,
    text=corr_matrix.values.round(2),
    texttemplate="%{text}",
    textfont={"size": 10}
))
fig_corr.update_layout(title='Interactive Correlation Matrix', width=800, height=800)
fig_corr.show()

fig_scatter = px.scatter(df, x='median_income', y='median_house_value',
                        title='Median Income vs Log Median House Value',
                        hover_data=['longitude', 'latitude'],
                        trendline='ols')
fig_scatter.show()

fig_geo = px.scatter_mapbox(df, lat='latitude', lon='longitude', color='median_house_value',
                            size='population', zoom=5, mapbox_style='open-street-map',
                            title='House Prices by Location',
                            color_continuous_scale=px.colors.sequential.Plasma)
fig_geo.show()

# 6. Prepare Data for Modeling
